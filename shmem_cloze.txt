In a {{c1::shared-memory programming model}} processes or threads have access to {{c2::shared and private memory space}};shared_memory models
In a {{c1::shared-memory system}} synchronization and communication between processes or threads is done via {{c2::a shared-memory programming model}};shared_memory models
{{c1::Caches}} are used to {{c2::amortize slow memory access times}} by {{c3::reusing temporarily stored values of previously accessed main memory locations}};shared_memory cache
{{c1::Caches}} leverage the principles of {{c2::spatial and temporal locality}};shared_memory cache
Improvements in {{c1::memory performance}} have not kept up with improvements in {{c2::processor performance}};shared_memory
A {{c1::cache}} consists of a {{c2::number of lines}} that {{c3::store blocks of memory}};shared_memory cache
A {{c1::cache miss}} occurs when {{c2::an address is not found in cache when reading/writing it}};shared_memory cache
The {{c1::recursive matrix matrix multiplication algorithm}} has a total work of {{c2::\[W(n) = 8 \cdot W\left(\frac{n}{2}\right) + O(n^2), W(1) = O(1)\]}} which is {{c3::\[W(n) = O(n^3)\]}} by the Master Theorem;shared_memory
The {{c1::recursive matrix matrix multiplication algorithm}} has a running time of {{c2::\[T(n) = T\left(\frac{n}{2}\right) + O(log\,n), T(1) = O(1)\]}} which is {{c3::\[T(n) = O(log^2\,n)\]}} by the Master Theorem;shared_memory
{{c1::Cache coherency}} can be provided by {{c2::a complex algorithm in the processor hardware}} called {{c3::cache coherence protocol}};shared_memory cache
{{c1::Cache coherence}} is maintained at the {{c2::cache line level}} and can be {{c3::update or invalidation based}};shared_memory cache
{{c1::False sharing}} occurs when {{c2::two or more cores hold the same block of memory in their private cache and update a part of it}} which can cause {{c3::a significant degrade in performance}};shared_memory cache
An alternative to {{c1::wasteful padding}} in order to avoid {{c2::false sharing}} is to {{c3::use local variables}};shared_memory cache
Modern {{c1::multi-processor systems}} are {{c2::highly NUMA}} because of {{c3::the hierarchical memory system}};shared_memory
In {{c1::multi-core systems}} each {{c2::core is only indirectly connected to main memory through memory controllers}} which make {{c3::access time highly non-uniform}};shared_memory
{{c1::First-touch OS support}} is when {{c2::a memory page is allocated close to the first core that touches it}};shared_memory
{{c1::Prefetching and multi-threading}} alleviate {{c2::the memory bottleneck}};shared_memory
{{c1::Super linear speed-up}} can be achieved by {{c2::the parallel algorithm leveraging the memory hierarchy better than the sequential version}};shared_memory
{{c1::Sequential consistency}} allows to {{c2::prove properties of parallel programs much like for sequential programs}};shared_memory
{{c1::Sequential consistency}} is typically {{c2::not guaranteed by modern multi-processors}};shared_memory
Special instructions like {{c1::memory fences or barriers}} {{c2::enforce pending memory operations to complete}};shared_memory
